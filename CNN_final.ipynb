{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from os import listdir\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b967a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import keras.metrics\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, dirs, files = next(os.walk(\"./Data_preprocessed/\")) #path with the preprocessed signals\n",
    "nod=len(dirs) #number of classes/folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the preprocessed signals\n",
    "\n",
    "# create empty list\n",
    "dataframes_list = list()\n",
    "labels_list = list()\n",
    "\n",
    "for j in range(nod):\n",
    "    c_path=os.path.join(path, dirs[j])\n",
    "    print(j)\n",
    "    print(c_path)\n",
    "    c_path, c_dirs, c_files = next(os.walk(c_path))\n",
    "    file_count = len(c_files)\n",
    "    print(file_count)\n",
    "    \n",
    "    # append datasets to the list\n",
    "    for i in range(file_count):\n",
    "        temp_df = pd.read_csv(c_path + '/' + c_files[i])\n",
    "        temp_df=temp_df.loc[:,~temp_df.columns.str.match(\"Unnamed\")]\n",
    "\n",
    "        dataframes_list.append(temp_df) # all signals\n",
    "        labels_list.append(j) #lebels of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sequences = []\n",
    "for one_seq in dataframes_list:\n",
    "    len_sequences.append(len(one_seq))\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict = {\n",
    "  0: \"P_Crossroad_Left\",\n",
    "  1: \"P_Crossroad_Right\",\n",
    "  2: \"P_Crossroad_Straight\",\n",
    "  3: \"P_Parking_Diagonal_Left\",\n",
    "  4: \"P_Parking_Diagonal_Right\",\n",
    "  5: \"P_Parking_Parallel_Left\",\n",
    "  6: \"P_Parking_Parallel_Right\",\n",
    "  7: \"P_Parking_Perpendicular_Left\",\n",
    "  8: \"P_Parking_Perpendicular_Right\",\n",
    "  9: \"P_Roundabout_Left\",\n",
    "  10: \"P_Roundabout_Right\",\n",
    "  11: \"P_Roundabout_Straight\",\n",
    "  12: \"S_Bending\",\n",
    "  13: \"S_Drinking\",\n",
    "  14: \"S_Eating\",\n",
    "  15: \"S_Turning_Back\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_target, test, test_target = prepare(dataframes_list, labels_list, data_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = make_model(input_shape=train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model_vtest.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0002\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model4.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history4 = model4.fit(\n",
    "    train,\n",
    "    train_target,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ee350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.models.load_model(\"best_model_v64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "accuracy_summ(history4)\n",
    "loss_summ(history4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on test set \n",
    "\n",
    "test_loss, test_acc = model4.evaluate(test, test_target)\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, f1-score for all classes\n",
    "\n",
    "results_log = classification_report(test_target, pred)\n",
    "print(results_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plots(train, train_target)\n",
    "scatter_plots(test, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_m(model4, test, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bafe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data for NN\n",
    "def prepare(dataframes_list, labels_list, data_dist=False):\n",
    "    dataframes_list_train, dataframes_list_test, labels_list_train, labels_list_test = train_test_split(dataframes_list, labels_list, test_size=0.1)\n",
    "    train = np.array(dataframes_list_train)\n",
    "    test = np.array(dataframes_list_test)\n",
    "    train_target = np.array(labels_list_train)\n",
    "    test_target = np.array(labels_list_test)\n",
    "    train, train_target = shuffle(train, train_target)\n",
    "    test, test_target = shuffle(test, test_target)\n",
    "    # data distribution\n",
    "    if data_dist==True:\n",
    "        plot_distribution(labels_list_train, dataframes_list_train)\n",
    "        plot_distribution(labels_list_test, dataframes_list_test)\n",
    "    return train, train_target, test, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736849c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining neural network architecture\n",
    "\n",
    "num_classes = len(np.unique(train_target))\n",
    "def make_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    conv1=keras.layers.MaxPooling1D(3)(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    \n",
    "    gap = keras.layers.Dropout(0.4)(gap)\n",
    "    \n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "    \n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model architecture\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model4, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "def accuracy_summ(history4):\n",
    "\n",
    "    plt.plot(history4.history['sparse_categorical_accuracy'])\n",
    "\n",
    "    plt.plot(history4.history['val_sparse_categorical_accuracy'])\n",
    "\n",
    "    plt.title('model accuracy')\n",
    "\n",
    "    plt.ylabel('accuracy')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "def loss_summ(history4):\n",
    "\n",
    "    plt.plot(history4.history['loss'])\n",
    "\n",
    "    plt.plot(history4.history['val_loss'])\n",
    "\n",
    "    plt.title('model loss')\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing confusion matrix\n",
    "\n",
    "from pretty_confusion_matrix import pp_matrix\n",
    "#model4 = keras.models.load_model(\"best_model_v64.h5\")\n",
    "def confusion_m(model4, t_set, target):\n",
    "    pred = model4.predict(t_set)\n",
    "    pred = pred.argmax(axis=-1)\n",
    "    cm=confusion_matrix(target,pred)\n",
    "    df_cm = pd.DataFrame(cm, index=range(0, 16), columns=range(0, 16))\n",
    "    cmap = 'icefire'\n",
    "    pred_val_axis ='lin'\n",
    "    pp_matrix(df_cm, cmap=cmap, pred_val_axis=pred_val_axis, fz=8, figsize=[12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ff4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "def scatter_plots(t_set, target):\n",
    "    pred_set = model4.predict(t_set)\n",
    "    pred_class = []\n",
    "    for i in range(len(pred_set)):\n",
    "        pred_class.append(np.argmax(pred_set[i]))\n",
    "    pca = PCA(n_components=2)\n",
    "    pts = pca.fit_transform(pred_set)\n",
    "    ax = plt.figure(figsize=(7,10))\n",
    "    sns.scatterplot(\n",
    "        x=pts[:,0], y=pts[:,1],\n",
    "        hue=target,\n",
    "        palette=sns.color_palette(\"viridis\", as_cmap=True),\n",
    "        legend=\"full\"\n",
    "    )\n",
    "    plt.xlabel('pca-one')\n",
    "    plt.ylabel('pca-two')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ax = plt.figure(figsize=(7,10)).gca(projection='3d')\n",
    "    ax.scatter(pts[:,0], pts[:,1], c = target, cmap=\"viridis\")\n",
    "    ax.set_xlabel('pca-one')\n",
    "    ax.set_ylabel('pca-two')\n",
    "    ax.set_zlabel('pca-three')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# function that plots the distribution of classes \n",
    "def plot_distribution(labels_list_train, dataframes_list_train):\n",
    "    labels = list(dict.fromkeys(labels_list_train))\n",
    "    labels=[thisdict.get(item,item)  for item in labels]\n",
    "    counts = pd.Series(labels_list_train).value_counts(sort=False).tolist()\n",
    "    \n",
    "    pie_plot = go.Pie(labels=labels, values=counts, hole=.3)\n",
    "    fig = go.Figure(data=[pie_plot])\n",
    "    fig.update_layout(title_text='Distribution of classes in the training set')\n",
    "    \n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
